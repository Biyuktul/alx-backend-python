
==> Audit <==
|-----------|-----------------------------|----------|----------------------------|---------|---------------------|---------------------|
|  Command  |            Args             | Profile  |            User            | Version |     Start Time      |      End Time       |
|-----------|-----------------------------|----------|----------------------------|---------|---------------------|---------------------|
| start     |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:39 EAT |                     |
| delete    |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:48 EAT | 25 Jun 25 08:48 EAT |
| start     | --memory=1024               | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:49 EAT |                     |
| start     | --memory=768                | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:49 EAT |                     |
| start     | --driver=wsl2 --memory=1024 | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:49 EAT |                     |
| start     | --driver=wsl2 --memory=1024 | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:49 EAT |                     |
| start     | --memory=1024               | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT |                     |
| start     | --memory=768                | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT |                     |
| delete    |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT | 25 Jun 25 08:52 EAT |
| delete    | *                           | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT |                     |
| start     | --memory=768                | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT |                     |
| start     | --driver=wsl2 --memory=1024 | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:52 EAT |                     |
| start     | --memory=2048               | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:53 EAT | 25 Jun 25 08:54 EAT |
| kubectl   | -- get pods -A              | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:55 EAT | 25 Jun 25 08:56 EAT |
| kubectl   | -- get pods -A              | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:56 EAT | 25 Jun 25 08:56 EAT |
| dashboard |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 08:56 EAT |                     |
| start     |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 25 Jun 25 09:11 EAT | 25 Jun 25 09:12 EAT |
| start     |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 27 Jun 25 08:33 EAT |                     |
| start     |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 27 Jun 25 08:34 EAT |                     |
| delete    |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 27 Jun 25 08:35 EAT | 27 Jun 25 08:35 EAT |
| start     |                             | minikube | OPTIPLEX7010\Administrator | v1.36.0 | 27 Jun 25 08:36 EAT | 27 Jun 25 08:42 EAT |
|-----------|-----------------------------|----------|----------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/27 08:36:02
Running on machine: Optiplex7010
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0627 08:36:02.094735   17032 out.go:345] Setting OutFile to fd 464 ...
I0627 08:36:02.094735   17032 out.go:392] TERM=xterm,COLORTERM=, which probably does not support color
I0627 08:36:02.094735   17032 out.go:358] Setting ErrFile to fd 464...
I0627 08:36:02.094735   17032 out.go:392] TERM=xterm,COLORTERM=, which probably does not support color
I0627 08:36:02.104816   17032 out.go:352] Setting JSON to false
I0627 08:36:02.107281   17032 start.go:130] hostinfo: {"hostname":"Optiplex7010","uptime":13477,"bootTime":1750989084,"procs":273,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.4484 Build 26100.4484","kernelVersion":"10.0.26100.4484 Build 26100.4484","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"a9ab6845-edbf-4388-9366-98f91d464591"}
W0627 08:36:02.107281   17032 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0627 08:36:02.109788   17032 out.go:177] * minikube v1.36.0 on Microsoft Windows 11 Pro 10.0.26100.4484 Build 26100.4484
I0627 08:36:02.110789   17032 notify.go:220] Checking for updates...
I0627 08:36:02.110789   17032 driver.go:404] Setting default libvirt URI to qemu:///system
I0627 08:36:02.110789   17032 global.go:112] Querying for installed drivers using PATH=C:\Users\Administrator\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\local\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\bin;C:\Users\Administrator\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\WINDOWS\System32\OpenSSH;C:\Program Files\dotnet;C:\Program Files\Git\cmd;C:\Users\Administrator\AppData\Local\Programs\Python\Python313;C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Scripts;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Kubernetes\Minikube;C:\Users\Administrator\AppData\Local\Programs\Python\Launcher;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;C:\Users\Administrator\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\Administrator\AppData\Local\Programs\Python\Python313;C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Scripts;C:\Users\Administrator\AppData\Local\GitHubDesktop\bin;C:\Program Files\Git\usr\bin\vendor_perl;C:\Program Files\Git\usr\bin\core_perl
I0627 08:36:02.115377   17032 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0627 08:36:02.115377   17032 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0627 08:36:04.485326   17032 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0627 08:36:04.488524   17032 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0627 08:36:04.494539   17032 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0627 08:36:04.497722   17032 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0627 08:36:04.583623   17032 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0627 08:36:04.585693   17032 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0627 08:36:04.962676   17032 info.go:266] docker info: {ID:c8fb75ef-eb69-425c-b6b8-71ce4aff3c0f Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:69 OomKillDisable:false NGoroutines:84 SystemTime:2025-06-27 05:36:04.941157801 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3943911424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0627 08:36:04.963198   17032 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0627 08:36:04.963198   17032 driver.go:326] not recommending "ssh" due to default: false
I0627 08:36:04.963198   17032 driver.go:361] Picked: docker
I0627 08:36:04.963198   17032 driver.go:362] Alternatives: [hyperv ssh]
I0627 08:36:04.963198   17032 driver.go:363] Rejects: [podman qemu2 virtualbox vmware]
I0627 08:36:04.963756   17032 out.go:177] * Automatically selected the docker driver. Other choices: hyperv, ssh
I0627 08:36:04.964786   17032 start.go:304] selected driver: docker
I0627 08:36:04.964786   17032 start.go:908] validating driver "docker" against <nil>
I0627 08:36:04.964786   17032 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0627 08:36:04.969049   17032 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0627 08:36:05.109519   17032 info.go:266] docker info: {ID:c8fb75ef-eb69-425c-b6b8-71ce4aff3c0f Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:69 OomKillDisable:false NGoroutines:84 SystemTime:2025-06-27 05:36:05.096124173 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3943911424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0627 08:36:05.110050   17032 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0627 08:36:05.131823   17032 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=7880MB, container=3761MB
I0627 08:36:05.132356   17032 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0627 08:36:05.133415   17032 out.go:177] * Using Docker Desktop driver with root privileges
I0627 08:36:05.134024   17032 cni.go:84] Creating CNI manager for ""
I0627 08:36:05.134024   17032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0627 08:36:05.134024   17032 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0627 08:36:05.134024   17032 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0627 08:36:05.134599   17032 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0627 08:36:05.135653   17032 cache.go:121] Beginning downloading kic base image for docker with docker
I0627 08:36:05.136187   17032 out.go:177] * Pulling base image v0.0.47 ...
I0627 08:36:05.137212   17032 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0627 08:36:05.137212   17032 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0627 08:36:05.137212   17032 preload.go:146] Found local preload: C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0627 08:36:05.137212   17032 cache.go:56] Caching tarball of preloaded images
I0627 08:36:05.137212   17032 preload.go:172] Found C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0627 08:36:05.137212   17032 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0627 08:36:05.137747   17032 profile.go:143] Saving config to C:\Users\Administrator\.minikube\profiles\minikube\config.json ...
I0627 08:36:05.137747   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\config.json: {Name:mk72497029ba331d03cccfebd8d8cfdf8706a2c1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:36:05.202339   17032 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0627 08:36:05.202872   17032 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0627 08:36:05.202872   17032 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0627 08:36:05.202872   17032 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0627 08:36:05.202872   17032 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0627 08:41:20.091470   17032 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0627 08:41:20.091470   17032 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0627 08:41:20.091470   17032 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0627 08:41:41.448262   17032 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0627 08:41:41.449290   17032 cache.go:230] Successfully downloaded all kic artifacts
I0627 08:41:41.453242   17032 start.go:360] acquireMachinesLock for minikube: {Name:mk3f259f80712de9f83f91ad0f12658ee47ef5bf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0627 08:41:41.454148   17032 start.go:364] duration metric: took 331.2µs to acquireMachinesLock for "minikube"
I0627 08:41:41.455176   17032 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0627 08:41:41.455704   17032 start.go:125] createHost starting for "" (driver="docker")
I0627 08:41:41.457988   17032 out.go:235] * Creating docker container (CPUs=2, Memory=2200MB) ...
I0627 08:41:41.463622   17032 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0627 08:41:41.464127   17032 client.go:168] LocalClient.Create starting
I0627 08:41:41.466993   17032 main.go:141] libmachine: Reading certificate data from C:\Users\Administrator\.minikube\certs\ca.pem
I0627 08:41:41.485220   17032 main.go:141] libmachine: Decoding PEM data...
I0627 08:41:41.485744   17032 main.go:141] libmachine: Parsing certificate...
I0627 08:41:41.488119   17032 main.go:141] libmachine: Reading certificate data from C:\Users\Administrator\.minikube\certs\cert.pem
I0627 08:41:41.498362   17032 main.go:141] libmachine: Decoding PEM data...
I0627 08:41:41.498362   17032 main.go:141] libmachine: Parsing certificate...
I0627 08:41:41.506344   17032 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0627 08:41:41.544366   17032 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0627 08:41:41.548257   17032 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0627 08:41:41.548257   17032 cli_runner.go:164] Run: docker network inspect minikube
W0627 08:41:41.584883   17032 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0627 08:41:41.584883   17032 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0627 08:41:41.585403   17032 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0627 08:41:41.588156   17032 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0627 08:41:41.633511   17032 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001d79b90}
I0627 08:41:41.634065   17032 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0627 08:41:41.637800   17032 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0627 08:41:41.755049   17032 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0627 08:41:41.755049   17032 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0627 08:41:41.765591   17032 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0627 08:41:41.809940   17032 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0627 08:41:41.846074   17032 oci.go:103] Successfully created a docker volume minikube
I0627 08:41:41.850697   17032 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0627 08:41:42.771522   17032 oci.go:107] Successfully prepared a docker volume minikube
I0627 08:41:42.771522   17032 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0627 08:41:42.771522   17032 kic.go:194] Starting extracting preloaded images to volume ...
I0627 08:41:42.775347   17032 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0627 08:41:48.466669   17032 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (5.691322s)
I0627 08:41:48.466669   17032 kic.go:203] duration metric: took 5.6951471s to extract preloaded images to volume ...
I0627 08:41:48.472022   17032 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0627 08:41:48.974494   17032 info.go:266] docker info: {ID:c8fb75ef-eb69-425c-b6b8-71ce4aff3c0f Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:5 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:72 OomKillDisable:false NGoroutines:89 SystemTime:2025-06-27 05:41:48.950683093 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3943911424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0627 08:41:48.982622   17032 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0627 08:41:49.118527   17032 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0627 08:41:49.509125   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0627 08:41:49.567894   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:41:49.617742   17032 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0627 08:41:49.694696   17032 oci.go:144] the created container "minikube" has a running status.
I0627 08:41:49.694696   17032 kic.go:225] Creating ssh key for kic: C:\Users\Administrator\.minikube\machines\minikube\id_rsa...
I0627 08:41:49.870663   17032 kic_runner.go:191] docker (temp): C:\Users\Administrator\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0627 08:41:50.009131   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:41:50.054560   17032 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0627 08:41:50.054560   17032 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0627 08:41:50.162366   17032 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Administrator\.minikube\machines\minikube\id_rsa...
I0627 08:41:51.999030   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:41:52.035717   17032 machine.go:93] provisionDockerMachine start ...
I0627 08:41:52.040556   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:52.077525   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:52.096121   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:52.096121   17032 main.go:141] libmachine: About to run SSH command:
hostname
I0627 08:41:52.230944   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0627 08:41:52.231993   17032 ubuntu.go:169] provisioning hostname "minikube"
I0627 08:41:52.234609   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:52.273648   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:52.273648   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:52.273648   17032 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0627 08:41:52.410288   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0627 08:41:52.414124   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:52.449008   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:52.449526   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:52.449526   17032 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0627 08:41:52.567444   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0627 08:41:52.567444   17032 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrator\.minikube CaCertPath:C:\Users\Administrator\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrator\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrator\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrator\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrator\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrator\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrator\.minikube}
I0627 08:41:52.567444   17032 ubuntu.go:177] setting up certificates
I0627 08:41:52.567967   17032 provision.go:84] configureAuth start
I0627 08:41:52.570588   17032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0627 08:41:52.605789   17032 provision.go:143] copyHostCerts
I0627 08:41:52.606820   17032 exec_runner.go:144] found C:\Users\Administrator\.minikube/ca.pem, removing ...
I0627 08:41:52.606820   17032 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\ca.pem
I0627 08:41:52.606820   17032 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\ca.pem --> C:\Users\Administrator\.minikube/ca.pem (1099 bytes)
I0627 08:41:52.607886   17032 exec_runner.go:144] found C:\Users\Administrator\.minikube/cert.pem, removing ...
I0627 08:41:52.607886   17032 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\cert.pem
I0627 08:41:52.607886   17032 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\cert.pem --> C:\Users\Administrator\.minikube/cert.pem (1139 bytes)
I0627 08:41:52.619437   17032 exec_runner.go:144] found C:\Users\Administrator\.minikube/key.pem, removing ...
I0627 08:41:52.619437   17032 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\key.pem
I0627 08:41:52.619951   17032 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\key.pem --> C:\Users\Administrator\.minikube/key.pem (1679 bytes)
I0627 08:41:52.620467   17032 provision.go:117] generating server cert: C:\Users\Administrator\.minikube\machines\server.pem ca-key=C:\Users\Administrator\.minikube\certs\ca.pem private-key=C:\Users\Administrator\.minikube\certs\ca-key.pem org=Administrator.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0627 08:41:52.860351   17032 provision.go:177] copyRemoteCerts
I0627 08:41:52.862944   17032 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0627 08:41:52.866630   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:52.903369   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:41:52.993351   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0627 08:41:53.016615   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0627 08:41:53.038588   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0627 08:41:53.060871   17032 provision.go:87] duration metric: took 492.3139ms to configureAuth
I0627 08:41:53.060871   17032 ubuntu.go:193] setting minikube options for container-runtime
I0627 08:41:53.062953   17032 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0627 08:41:53.065554   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:53.107912   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:53.108438   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:53.108438   17032 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0627 08:41:53.215418   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0627 08:41:53.215418   17032 ubuntu.go:71] root file system type: overlay
I0627 08:41:53.215418   17032 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0627 08:41:53.215418   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:53.273784   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:53.273784   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:53.273784   17032 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0627 08:41:53.408246   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0627 08:41:53.410953   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:53.445684   17032 main.go:141] libmachine: Using SSH client type: native
I0627 08:41:53.445684   17032 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x111a9e0] 0x111d520 <nil>  [] 0s} 127.0.0.1 51672 <nil> <nil>}
I0627 08:41:53.445684   17032 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0627 08:41:55.300660   17032 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-06-27 05:41:53.404184809 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0627 08:41:55.300660   17032 machine.go:96] duration metric: took 3.2649425s to provisionDockerMachine
I0627 08:41:55.301164   17032 client.go:171] duration metric: took 13.8370373s to LocalClient.Create
I0627 08:41:55.301164   17032 start.go:167] duration metric: took 13.8375422s to libmachine.API.Create "minikube"
I0627 08:41:55.301164   17032 start.go:293] postStartSetup for "minikube" (driver="docker")
I0627 08:41:55.301690   17032 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0627 08:41:55.303265   17032 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0627 08:41:55.305422   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:55.337046   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:41:55.427657   17032 ssh_runner.go:195] Run: cat /etc/os-release
I0627 08:41:55.431634   17032 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0627 08:41:55.431634   17032 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0627 08:41:55.431634   17032 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0627 08:41:55.431634   17032 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0627 08:41:55.431634   17032 filesync.go:126] Scanning C:\Users\Administrator\.minikube\addons for local assets ...
I0627 08:41:55.432663   17032 filesync.go:126] Scanning C:\Users\Administrator\.minikube\files for local assets ...
I0627 08:41:55.432663   17032 start.go:296] duration metric: took 130.9732ms for postStartSetup
I0627 08:41:55.436902   17032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0627 08:41:55.468251   17032 profile.go:143] Saving config to C:\Users\Administrator\.minikube\profiles\minikube\config.json ...
I0627 08:41:55.470335   17032 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0627 08:41:55.472931   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:55.504098   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:41:55.595578   17032 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0627 08:41:55.600809   17032 start.go:128] duration metric: took 14.144585s to createHost
I0627 08:41:55.600809   17032 start.go:83] releasing machines lock for "minikube", held for 14.1466613s
I0627 08:41:55.603022   17032 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0627 08:41:55.634339   17032 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0627 08:41:55.634858   17032 ssh_runner.go:195] Run: cat /version.json
I0627 08:41:55.637966   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:55.637966   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:41:55.669181   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:41:55.681071   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
W0627 08:41:55.766224   17032 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0627 08:41:55.776275   17032 ssh_runner.go:195] Run: systemctl --version
I0627 08:41:55.781459   17032 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0627 08:41:55.787280   17032 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0627 08:41:55.794059   17032 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0627 08:41:55.795620   17032 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0627 08:41:55.818295   17032 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0627 08:41:55.818295   17032 start.go:495] detecting cgroup driver to use...
I0627 08:41:55.818295   17032 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0627 08:41:55.819848   17032 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0627 08:41:55.839395   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0627 08:41:55.849044   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0627 08:41:55.857504   17032 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0627 08:41:55.858025   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0627 08:41:55.867172   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0627 08:41:55.875456   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0627 08:41:55.883963   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0627 08:41:55.892283   17032 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0627 08:41:55.900634   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0627 08:41:55.908610   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0627 08:41:55.917075   17032 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0627 08:41:55.926067   17032 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0627 08:41:55.934389   17032 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0627 08:41:55.942264   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:41:55.980251   17032 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0627 08:41:56.062046   17032 start.go:495] detecting cgroup driver to use...
I0627 08:41:56.062046   17032 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0627 08:41:56.064160   17032 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0627 08:41:56.074655   17032 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0627 08:41:56.076219   17032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0627 08:41:56.085964   17032 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0627 08:41:56.100828   17032 ssh_runner.go:195] Run: which cri-dockerd
I0627 08:41:56.105550   17032 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0627 08:41:56.113529   17032 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0627 08:41:56.127010   17032 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0627 08:41:56.169116   17032 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0627 08:41:56.230635   17032 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0627 08:41:56.233243   17032 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0627 08:41:56.248086   17032 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0627 08:41:56.258794   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:41:56.304167   17032 ssh_runner.go:195] Run: sudo systemctl restart docker
W0627 08:41:56.855003   17032 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0627 08:41:56.856045   17032 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0627 08:41:58.793860   17032 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.4896938s)
I0627 08:41:58.796126   17032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0627 08:41:58.808051   17032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0627 08:41:58.820836   17032 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0627 08:41:58.866600   17032 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0627 08:41:58.948367   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:41:59.030717   17032 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0627 08:41:59.065089   17032 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0627 08:41:59.082714   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:41:59.141305   17032 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0627 08:41:59.237324   17032 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0627 08:41:59.245665   17032 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0627 08:41:59.246704   17032 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0627 08:41:59.249833   17032 start.go:563] Will wait 60s for crictl version
I0627 08:41:59.250386   17032 ssh_runner.go:195] Run: which crictl
I0627 08:41:59.254729   17032 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0627 08:41:59.278576   17032 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0627 08:41:59.280635   17032 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0627 08:41:59.301690   17032 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0627 08:41:59.319976   17032 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0627 08:41:59.322542   17032 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0627 08:41:59.421013   17032 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0627 08:41:59.422218   17032 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0627 08:41:59.425372   17032 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0627 08:41:59.436456   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0627 08:41:59.469550   17032 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0627 08:41:59.470067   17032 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0627 08:41:59.472187   17032 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0627 08:41:59.486707   17032 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0627 08:41:59.486707   17032 docker.go:632] Images already preloaded, skipping extraction
I0627 08:41:59.489806   17032 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0627 08:41:59.502777   17032 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0627 08:41:59.502777   17032 cache_images.go:84] Images are preloaded, skipping loading
I0627 08:41:59.502777   17032 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0627 08:41:59.503812   17032 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0627 08:41:59.506419   17032 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0627 08:41:59.541158   17032 cni.go:84] Creating CNI manager for ""
I0627 08:41:59.541158   17032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0627 08:41:59.541682   17032 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0627 08:41:59.541682   17032 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0627 08:41:59.542720   17032 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0627 08:41:59.543758   17032 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0627 08:41:59.551117   17032 binaries.go:44] Found k8s binaries, skipping transfer
I0627 08:41:59.552673   17032 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0627 08:41:59.559385   17032 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0627 08:41:59.570650   17032 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0627 08:41:59.583802   17032 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0627 08:41:59.599282   17032 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0627 08:41:59.602967   17032 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0627 08:41:59.614299   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:41:59.676788   17032 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0627 08:41:59.709598   17032 certs.go:68] Setting up C:\Users\Administrator\.minikube\profiles\minikube for IP: 192.168.49.2
I0627 08:41:59.709598   17032 certs.go:194] generating shared ca certs ...
I0627 08:41:59.709598   17032 certs.go:226] acquiring lock for ca certs: {Name:mk38b1aba3ca636999c633fc59ac7bca24a1dea0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:41:59.723244   17032 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Administrator\.minikube\ca.key
I0627 08:41:59.740130   17032 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Administrator\.minikube\proxy-client-ca.key
I0627 08:41:59.740130   17032 certs.go:256] generating profile certs ...
I0627 08:41:59.741155   17032 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Administrator\.minikube\profiles\minikube\client.key
I0627 08:41:59.741695   17032 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\client.crt with IP's: []
I0627 08:41:59.950418   17032 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\client.crt ...
I0627 08:41:59.952797   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\client.crt: {Name:mkcc9b0ab8e6e5009e256262a102e33f6b4c5b0c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:41:59.952797   17032 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\client.key ...
I0627 08:41:59.952797   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\client.key: {Name:mk30c85153bb8b579f384eb94bb7d3e1a95d37c1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:41:59.955696   17032 certs.go:363] generating signed profile cert for "minikube": C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0627 08:41:59.955696   17032 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0627 08:41:59.977593   17032 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0627 08:41:59.977593   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk42e86e594c0431370d9a4fdf2a5a41f74706b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:41:59.977593   17032 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0627 08:41:59.977593   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mke7b9551efbfa0955358538d27b6637a4c3a01f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:41:59.977593   17032 certs.go:381] copying C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt
I0627 08:41:59.989901   17032 certs.go:385] copying C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key
I0627 08:41:59.989901   17032 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key
I0627 08:41:59.989901   17032 crypto.go:68] Generating cert C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0627 08:42:00.039277   17032 crypto.go:156] Writing cert to C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt ...
I0627 08:42:00.039277   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt: {Name:mk0e7578614ca016556a64bcd6d1ae8f43f6c356 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:42:00.039277   17032 crypto.go:164] Writing key to C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key ...
I0627 08:42:00.039277   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key: {Name:mk2b6069b395c3db8e1dc812226871b6c6a0d814 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:42:00.049736   17032 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca-key.pem (1679 bytes)
I0627 08:42:00.049736   17032 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca.pem (1099 bytes)
I0627 08:42:00.049736   17032 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\cert.pem (1139 bytes)
I0627 08:42:00.049736   17032 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\key.pem (1679 bytes)
I0627 08:42:00.063879   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0627 08:42:00.082935   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0627 08:42:00.098925   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0627 08:42:00.114699   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0627 08:42:00.130694   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0627 08:42:00.146630   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0627 08:42:00.162629   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0627 08:42:00.178524   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0627 08:42:00.194879   17032 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0627 08:42:00.211407   17032 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0627 08:42:00.223654   17032 ssh_runner.go:195] Run: openssl version
I0627 08:42:00.228648   17032 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0627 08:42:00.237602   17032 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0627 08:42:00.240725   17032 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 25 05:54 /usr/share/ca-certificates/minikubeCA.pem
I0627 08:42:00.240725   17032 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0627 08:42:00.247012   17032 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0627 08:42:00.253998   17032 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0627 08:42:00.257107   17032 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0627 08:42:00.257107   17032 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0627 08:42:00.259691   17032 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0627 08:42:00.273189   17032 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0627 08:42:00.281239   17032 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0627 08:42:00.287583   17032 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0627 08:42:00.289165   17032 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0627 08:42:00.295536   17032 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0627 08:42:00.295536   17032 kubeadm.go:157] found existing configuration files:

I0627 08:42:00.297152   17032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0627 08:42:00.303427   17032 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0627 08:42:00.304453   17032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0627 08:42:00.311903   17032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0627 08:42:00.316063   17032 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0627 08:42:00.316063   17032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0627 08:42:00.326186   17032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0627 08:42:00.332613   17032 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0627 08:42:00.333657   17032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0627 08:42:00.340953   17032 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0627 08:42:00.347734   17032 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0627 08:42:00.349301   17032 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0627 08:42:00.357251   17032 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0627 08:42:00.508255   17032 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0627 08:42:00.587820   17032 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0627 08:42:10.813659   17032 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0627 08:42:10.813659   17032 kubeadm.go:310] [preflight] Running pre-flight checks
I0627 08:42:10.813659   17032 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0627 08:42:10.813659   17032 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0627 08:42:10.813659   17032 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0627 08:42:10.814189   17032 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0627 08:42:10.815222   17032 out.go:235]   - Generating certificates and keys ...
I0627 08:42:10.815222   17032 kubeadm.go:310] [certs] Using existing ca certificate authority
I0627 08:42:10.815222   17032 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0627 08:42:10.815222   17032 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0627 08:42:10.815737   17032 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0627 08:42:10.816249   17032 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0627 08:42:10.816249   17032 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0627 08:42:10.816249   17032 kubeadm.go:310] [certs] Generating "sa" key and public key
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0627 08:42:10.816249   17032 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0627 08:42:10.816766   17032 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0627 08:42:10.816766   17032 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0627 08:42:10.817279   17032 out.go:235]   - Booting up control plane ...
I0627 08:42:10.817279   17032 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0627 08:42:10.817279   17032 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0627 08:42:10.817796   17032 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0627 08:42:10.817796   17032 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0627 08:42:10.817796   17032 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0627 08:42:10.817796   17032 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0627 08:42:10.817796   17032 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0627 08:42:10.817796   17032 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0627 08:42:10.818313   17032 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.148013ms
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 3.428416127s
I0627 08:42:10.818313   17032 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 4.335302864s
I0627 08:42:10.818827   17032 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 6.002601479s
I0627 08:42:10.818827   17032 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0627 08:42:10.818827   17032 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0627 08:42:10.818827   17032 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0627 08:42:10.818827   17032 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0627 08:42:10.819343   17032 kubeadm.go:310] [bootstrap-token] Using token: dnr5bx.amlyoxj59qstl2eo
I0627 08:42:10.820383   17032 out.go:235]   - Configuring RBAC rules ...
I0627 08:42:10.820383   17032 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0627 08:42:10.820383   17032 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0627 08:42:10.820383   17032 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0627 08:42:10.820897   17032 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0627 08:42:10.820897   17032 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0627 08:42:10.820897   17032 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0627 08:42:10.820897   17032 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0627 08:42:10.820897   17032 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0627 08:42:10.820897   17032 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0627 08:42:10.820897   17032 kubeadm.go:310] 
I0627 08:42:10.820897   17032 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0627 08:42:10.820897   17032 kubeadm.go:310] 
I0627 08:42:10.821408   17032 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0627 08:42:10.821408   17032 kubeadm.go:310] 
I0627 08:42:10.821408   17032 kubeadm.go:310]   mkdir -p $HOME/.kube
I0627 08:42:10.821408   17032 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0627 08:42:10.821408   17032 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0627 08:42:10.821408   17032 kubeadm.go:310] 
I0627 08:42:10.821408   17032 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0627 08:42:10.821408   17032 kubeadm.go:310] 
I0627 08:42:10.821408   17032 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0627 08:42:10.821408   17032 kubeadm.go:310] 
I0627 08:42:10.821408   17032 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0627 08:42:10.821408   17032 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0627 08:42:10.821919   17032 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0627 08:42:10.821919   17032 kubeadm.go:310] 
I0627 08:42:10.821919   17032 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0627 08:42:10.821919   17032 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0627 08:42:10.821919   17032 kubeadm.go:310] 
I0627 08:42:10.821919   17032 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token dnr5bx.amlyoxj59qstl2eo \
I0627 08:42:10.821919   17032 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:0826bfd5a8883501bdb12893b28a5226137de1e6801abf4f23b96db6940caa0c \
I0627 08:42:10.821919   17032 kubeadm.go:310] 	--control-plane 
I0627 08:42:10.821919   17032 kubeadm.go:310] 
I0627 08:42:10.821919   17032 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0627 08:42:10.821919   17032 kubeadm.go:310] 
I0627 08:42:10.822436   17032 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token dnr5bx.amlyoxj59qstl2eo \
I0627 08:42:10.822436   17032 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:0826bfd5a8883501bdb12893b28a5226137de1e6801abf4f23b96db6940caa0c 
I0627 08:42:10.822436   17032 cni.go:84] Creating CNI manager for ""
I0627 08:42:10.822436   17032 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0627 08:42:10.823457   17032 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0627 08:42:10.825606   17032 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0627 08:42:10.833244   17032 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0627 08:42:10.847795   17032 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0627 08:42:10.850612   17032 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_06_27T08_42_10_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0627 08:42:10.850612   17032 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0627 08:42:10.856302   17032 ops.go:34] apiserver oom_adj: -16
I0627 08:42:11.035229   17032 kubeadm.go:1105] duration metric: took 186.9299ms to wait for elevateKubeSystemPrivileges
I0627 08:42:11.035229   17032 kubeadm.go:394] duration metric: took 10.778122s to StartCluster
I0627 08:42:11.035229   17032 settings.go:142] acquiring lock: {Name:mk1dc43883b192668fc34902159f52a719564aae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:42:11.035737   17032 settings.go:150] Updating kubeconfig:  C:\Users\Administrator\.kube\config
I0627 08:42:11.039917   17032 lock.go:35] WriteFile acquiring C:\Users\Administrator\.kube\config: {Name:mkea657c15dc45de69286aed2fa89967aee013e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0627 08:42:11.041468   17032 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0627 08:42:11.041468   17032 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0627 08:42:11.041468   17032 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0627 08:42:11.041988   17032 out.go:177] * Verifying Kubernetes components...
I0627 08:42:11.041988   17032 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0627 08:42:11.043031   17032 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0627 08:42:11.043031   17032 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0627 08:42:11.043031   17032 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0627 08:42:11.043031   17032 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0627 08:42:11.043549   17032 host.go:66] Checking if "minikube" exists ...
I0627 08:42:11.045120   17032 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0627 08:42:11.052025   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:42:11.052548   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:42:11.094067   17032 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0627 08:42:11.095109   17032 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0627 08:42:11.095109   17032 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0627 08:42:11.097785   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:42:11.120376   17032 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0627 08:42:11.120905   17032 host.go:66] Checking if "minikube" exists ...
I0627 08:42:11.125932   17032 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0627 08:42:11.128017   17032 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0627 08:42:11.130118   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:42:11.137692   17032 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0627 08:42:11.164803   17032 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0627 08:42:11.164803   17032 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0627 08:42:11.167433   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0627 08:42:11.199393   17032 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51672 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0627 08:42:11.516741   17032 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0627 08:42:11.528450   17032 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0627 08:42:11.636815   17032 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0627 08:42:11.640049   17032 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0627 08:42:11.675409   17032 api_server.go:52] waiting for apiserver process to appear ...
I0627 08:42:11.676460   17032 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0627 08:42:11.946870   17032 api_server.go:72] duration metric: took 905.402ms to wait for apiserver process to appear ...
I0627 08:42:11.946870   17032 api_server.go:88] waiting for apiserver healthz status ...
I0627 08:42:11.946870   17032 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51676/healthz ...
I0627 08:42:11.952138   17032 api_server.go:279] https://127.0.0.1:51676/healthz returned 200:
ok
I0627 08:42:11.954231   17032 api_server.go:141] control plane version: v1.33.1
I0627 08:42:11.954231   17032 api_server.go:131] duration metric: took 7.3611ms to wait for apiserver health ...
I0627 08:42:11.954231   17032 system_pods.go:43] waiting for kube-system pods to appear ...
I0627 08:42:11.957342   17032 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0627 08:42:11.958372   17032 addons.go:514] duration metric: took 916.9044ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0627 08:42:11.962534   17032 system_pods.go:59] 5 kube-system pods found
I0627 08:42:11.962534   17032 system_pods.go:61] "etcd-minikube" [d9e85ee0-a61c-4d1f-bd20-511ae0c23a4b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0627 08:42:11.962534   17032 system_pods.go:61] "kube-apiserver-minikube" [f8064277-e49a-44ce-883a-ff454c0bc760] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0627 08:42:11.963044   17032 system_pods.go:61] "kube-controller-manager-minikube" [fdfc4440-671b-49cc-8da9-e14bc75a0472] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0627 08:42:11.963044   17032 system_pods.go:61] "kube-scheduler-minikube" [4f5165bd-a04f-4a49-8425-30222eee0ecc] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0627 08:42:11.963044   17032 system_pods.go:61] "storage-provisioner" [6accb74d-1aeb-431d-9888-a890827b08d5] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0627 08:42:11.963044   17032 system_pods.go:74] duration metric: took 8.8136ms to wait for pod list to return data ...
I0627 08:42:11.963044   17032 kubeadm.go:578] duration metric: took 921.5767ms to wait for: map[apiserver:true system_pods:true]
I0627 08:42:11.963044   17032 node_conditions.go:102] verifying NodePressure condition ...
I0627 08:42:11.965813   17032 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0627 08:42:11.965813   17032 node_conditions.go:123] node cpu capacity is 12
I0627 08:42:11.966320   17032 node_conditions.go:105] duration metric: took 3.2757ms to run NodePressure ...
I0627 08:42:11.966320   17032 start.go:241] waiting for startup goroutines ...
I0627 08:42:12.145037   17032 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0627 08:42:12.145037   17032 start.go:246] waiting for cluster config update ...
I0627 08:42:12.145037   17032 start.go:255] writing updated cluster config ...
I0627 08:42:12.150760   17032 ssh_runner.go:195] Run: rm -f paused
I0627 08:42:12.216753   17032 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0627 08:42:12.217783   17032 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 27 05:41:55 minikube dockerd[686]: time="2025-06-27T05:41:55.990188177Z" level=info msg="Daemon shutdown complete"
Jun 27 05:41:55 minikube dockerd[686]: time="2025-06-27T05:41:55.990228367Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jun 27 05:41:55 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 27 05:41:55 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 27 05:41:56 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.085742241Z" level=info msg="Starting up"
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.086545309Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.095310607Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.110408880Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.112666979Z" level=info msg="Loading containers: start."
Jun 27 05:41:56 minikube dockerd[1032]: time="2025-06-27T05:41:56.312744813Z" level=info msg="Processing signal 'terminated'"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.219529446Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count a64bf6819412c328b6b3242a6e5727ac8f2351c6bbd1fe58c86121b16e4cc3b0], retrying...."
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.255987307Z" level=info msg="Loading containers: done."
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.265859549Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.265905476Z" level=info msg="Initializing buildkit"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.280186251Z" level=info msg="Completed buildkit initialization"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.283521302Z" level=info msg="Daemon has completed initialization"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.283592069Z" level=info msg="API listen on [::]:2376"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.283715958Z" level=info msg="API listen on /var/run/docker.sock"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.285829575Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.286087939Z" level=info msg="Daemon shutdown complete"
Jun 27 05:41:57 minikube dockerd[1032]: time="2025-06-27T05:41:57.286122530Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jun 27 05:41:57 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 27 05:41:57 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 27 05:41:57 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 27 05:41:57 minikube dockerd[1339]: time="2025-06-27T05:41:57.350032293Z" level=info msg="Starting up"
Jun 27 05:41:57 minikube dockerd[1339]: time="2025-06-27T05:41:57.351270619Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 27 05:41:57 minikube dockerd[1339]: time="2025-06-27T05:41:57.358174093Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 27 05:41:57 minikube dockerd[1339]: time="2025-06-27T05:41:57.364864263Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 27 05:41:57 minikube dockerd[1339]: time="2025-06-27T05:41:57.373550949Z" level=info msg="Loading containers: start."
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.732678813Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count c08f8ec66dbfb8453da7c723399c8e70c5cf85545e21bb4d7c9ae93225e75729], retrying...."
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.768048823Z" level=info msg="Loading containers: done."
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.775646934Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.775682683Z" level=info msg="Initializing buildkit"
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.789301830Z" level=info msg="Completed buildkit initialization"
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.793122411Z" level=info msg="Daemon has completed initialization"
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.793186602Z" level=info msg="API listen on [::]:2376"
Jun 27 05:41:58 minikube dockerd[1339]: time="2025-06-27T05:41:58.793198945Z" level=info msg="API listen on /var/run/docker.sock"
Jun 27 05:41:58 minikube systemd[1]: Started Docker Application Container Engine.
Jun 27 05:41:59 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Start docker client with request timeout 0s"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Loaded network plugin cni"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 27 05:41:59 minikube cri-dockerd[1647]: time="2025-06-27T05:41:59Z" level=info msg="Start cri-dockerd grpc backend"
Jun 27 05:41:59 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 27 05:42:03 minikube cri-dockerd[1647]: time="2025-06-27T05:42:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95f18c33779de3b59e6b4768c7d6c08045d151b667699a0a16ef9cb718f80da1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:03 minikube cri-dockerd[1647]: time="2025-06-27T05:42:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/66eb9aa470a3aa97d74a4a573bf17fc87c264d0ad05bbe27e58248b78c0ed16d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:03 minikube cri-dockerd[1647]: time="2025-06-27T05:42:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a7c542d0e356346826639058ede4442cee84899dcecff60d40532e4877d1b73/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:03 minikube cri-dockerd[1647]: time="2025-06-27T05:42:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/26675fc0d8686ebe67dce44dbfcc5e87dece278e44fe7ff898c4b47f3d7fbc48/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:14 minikube cri-dockerd[1647]: time="2025-06-27T05:42:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cf2b80728e8121b2dc6df56bdcdce25c58d2136d35df8890ba327a15516da6a8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:15 minikube cri-dockerd[1647]: time="2025-06-27T05:42:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/946d2bce7c5f0f4a62544978311b8d02a1679c39cc7b2c2d22311c09dc22c15e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:15 minikube cri-dockerd[1647]: time="2025-06-27T05:42:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e70f084476d72a3167482e873459a20b92c6897651fe871277f5681f7e636bef/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 27 05:42:20 minikube cri-dockerd[1647]: time="2025-06-27T05:42:20Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 27 05:42:35 minikube dockerd[1339]: time="2025-06-27T05:42:35.902621899Z" level=info msg="ignoring event" container=aa3c63239adfd3644aa070b3e1ec090ea61a555c615038a3be6b4960e9751ade module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8b5621dd30894       6e38f40d628db       3 minutes ago       Running             storage-provisioner       1                   cf2b80728e812       storage-provisioner
b5de041c8a94d       1cf5f116067c6       3 minutes ago       Running             coredns                   0                   e70f084476d72       coredns-674b8bbfcf-scg9h
53ddba9a2c19a       b79c189b052cd       3 minutes ago       Running             kube-proxy                0                   946d2bce7c5f0       kube-proxy-jwxnm
aa3c63239adfd       6e38f40d628db       3 minutes ago       Exited              storage-provisioner       0                   cf2b80728e812       storage-provisioner
a63c1ec9fabcb       398c985c0d950       3 minutes ago       Running             kube-scheduler            0                   26675fc0d8686       kube-scheduler-minikube
1fa7f2cccacc0       ef43894fa110c       3 minutes ago       Running             kube-controller-manager   0                   4a7c542d0e356       kube-controller-manager-minikube
a444dfac0186e       499038711c081       3 minutes ago       Running             etcd                      0                   66eb9aa470a3a       etcd-minikube
29bbb5a0ee30e       c6ab243b29f82       3 minutes ago       Running             kube-apiserver            0                   95f18c33779de       kube-apiserver-minikube


==> coredns [b5de041c8a94] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:56605 - 22849 "HINFO IN 3505300705148316181.6819430458389390300. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.038907497s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_27T08_42_10_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 27 Jun 2025 05:42:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 27 Jun 2025 05:45:44 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 27 Jun 2025 05:42:20 +0000   Fri, 27 Jun 2025 05:42:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 27 Jun 2025 05:42:20 +0000   Fri, 27 Jun 2025 05:42:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 27 Jun 2025 05:42:20 +0000   Fri, 27 Jun 2025 05:42:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 27 Jun 2025 05:42:20 +0000   Fri, 27 Jun 2025 05:42:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3851476Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3851476Ki
  pods:               110
System Info:
  Machine ID:                 786d90715ac549ca9c46aa6940574a63
  System UUID:                786d90715ac549ca9c46aa6940574a63
  Boot ID:                    0f9f26a7-aaae-4cc2-a7b7-fc9a0a730f75
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-674b8bbfcf-scg9h            100m (0%)     0 (0%)      70Mi (1%)        170Mi (4%)     3m30s
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (2%)       0 (0%)         3m35s
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         3m35s
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         3m35s
  kube-system                 kube-proxy-jwxnm                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m30s
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         3m35s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m34s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 3m29s                  kube-proxy       
  Normal  NodeHasSufficientMemory  3m42s (x8 over 3m42s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m42s (x8 over 3m42s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m42s (x7 over 3m42s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m42s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 3m35s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  3m35s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  3m35s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m35s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m35s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           3m31s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun27 05:29] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.025519] PCI: Fatal: No config space access function found
[  +0.024901] PCI: System does not support PCI
[  +0.078744] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.871257] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.011021] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Africa/Nairobi not found. Is the tzdata package installed?
[  +0.126826] pulseaudio[278]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.100548] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002770] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000454] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000395] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000699] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.538556] netlink: 'init': attribute type 4 has an invalid length.
[  +4.656103] WSL (239) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [a444dfac0186] <==
{"level":"warn","ts":"2025-06-27T05:42:04.525248Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-06-27T05:42:04.525399Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-06-27T05:42:04.525439Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-06-27T05:42:04.525611Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-27T05:42:04.525637Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-27T05:42:04.525695Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-27T05:42:04.526631Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-27T05:42:04.526795Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-27T05:42:04.531412Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.710727ms"}
{"level":"info","ts":"2025-06-27T05:42:04.542623Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-06-27T05:42:04.542758Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-06-27T05:42:04.542797Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-06-27T05:42:04.542803Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-06-27T05:42:04.542807Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-06-27T05:42:04.542852Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-06-27T05:42:04.620920Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-27T05:42:04.624991Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-06-27T05:42:04.625045Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-06-27T05:42:04.628129Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-27T05:42:04.633761Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-06-27T05:42:04.633906Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-27T05:42:04.633991Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-27T05:42:04.634105Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-27T05:42:04.634114Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-27T05:42:04.634677Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-27T05:42:04.635270Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-27T05:42:04.635394Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-06-27T05:42:04.637853Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-27T05:42:04.637933Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-27T05:42:04.637979Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-27T05:42:04.638123Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-27T05:42:04.638143Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-27T05:42:04.743416Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-06-27T05:42:04.743452Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-06-27T05:42:04.743463Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-06-27T05:42:04.743612Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-06-27T05:42:04.743628Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-27T05:42:04.743633Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-06-27T05:42:04.743637Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-27T05:42:04.744640Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-27T05:42:04.745689Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-27T05:42:04.745705Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-27T05:42:04.745728Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-27T05:42:04.745980Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-27T05:42:04.746017Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-27T05:42:04.746218Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-27T05:42:04.746590Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-27T05:42:04.746631Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-27T05:42:04.747371Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-27T05:42:04.751135Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-27T05:42:04.751208Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-27T05:42:04.751221Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 05:45:45 up 16 min,  0 users,  load average: 0.05, 0.15, 0.08
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [29bbb5a0ee30] <==
I0627 05:42:07.535951       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0627 05:42:07.536279       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0627 05:42:07.536459       1 controller.go:119] Starting legacy_token_tracking_controller
I0627 05:42:07.536469       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0627 05:42:07.535215       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0627 05:42:07.537552       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0627 05:42:07.537635       1 repairip.go:200] Starting ipallocator-repair-controller
I0627 05:42:07.537641       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0627 05:42:07.539642       1 establishing_controller.go:81] Starting EstablishingController
I0627 05:42:07.539699       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0627 05:42:07.539711       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0627 05:42:07.539727       1 crd_finalizer.go:269] Starting CRDFinalizer
I0627 05:42:07.540226       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0627 05:42:07.540254       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0627 05:42:07.540286       1 aggregator.go:169] waiting for initial CRD sync...
I0627 05:42:07.540327       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0627 05:42:07.540331       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
E0627 05:42:07.621060       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0627 05:42:07.707568       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0627 05:42:07.707840       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0627 05:42:07.707845       1 cache.go:39] Caches are synced for LocalAvailability controller
I0627 05:42:07.707948       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0627 05:42:07.708871       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0627 05:42:07.708873       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0627 05:42:07.708889       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0627 05:42:07.708898       1 aggregator.go:171] initial CRD sync complete...
I0627 05:42:07.708905       1 autoregister_controller.go:144] Starting autoregister controller
I0627 05:42:07.708910       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0627 05:42:07.708916       1 cache.go:39] Caches are synced for autoregister controller
I0627 05:42:07.708922       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0627 05:42:07.709284       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0627 05:42:07.709297       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0627 05:42:07.709526       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0627 05:42:07.709549       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0627 05:42:07.710697       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0627 05:42:07.710731       1 policy_source.go:240] refreshing policies
I0627 05:42:07.710785       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0627 05:42:07.717122       1 controller.go:667] quota admission added evaluator for: namespaces
I0627 05:42:07.719915       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0627 05:42:07.720150       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0627 05:42:07.725631       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0627 05:42:07.725957       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0627 05:42:07.825058       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0627 05:42:08.544410       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0627 05:42:08.549992       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0627 05:42:08.550015       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0627 05:42:09.013991       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0627 05:42:09.055118       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0627 05:42:09.225202       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0627 05:42:09.231644       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0627 05:42:09.232284       1 controller.go:667] quota admission added evaluator for: endpoints
I0627 05:42:09.235603       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0627 05:42:09.565875       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0627 05:42:10.206965       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0627 05:42:10.221940       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0627 05:42:10.229350       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0627 05:42:14.812084       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0627 05:42:14.969626       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0627 05:42:14.971627       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0627 05:42:15.263301       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps


==> kube-controller-manager [1fa7f2cccacc] <==
I0627 05:42:14.020656       1 controllermanager.go:778] "Started controller" controller="replicaset-controller"
I0627 05:42:14.020805       1 replica_set.go:219] "Starting controller" logger="replicaset-controller" name="replicaset"
I0627 05:42:14.020832       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicaSet"
I0627 05:42:14.164924       1 controllermanager.go:778] "Started controller" controller="cronjob-controller"
I0627 05:42:14.165438       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I0627 05:42:14.165470       1 shared_informer.go:350] "Waiting for caches to sync" controller="cronjob"
I0627 05:42:14.169072       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0627 05:42:14.171418       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0627 05:42:14.178025       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0627 05:42:14.192370       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0627 05:42:14.209007       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0627 05:42:14.209460       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0627 05:42:14.212532       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0627 05:42:14.212582       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0627 05:42:14.212623       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0627 05:42:14.213911       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0627 05:42:14.213943       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0627 05:42:14.215256       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0627 05:42:14.215421       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0627 05:42:14.216030       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0627 05:42:14.216594       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0627 05:42:14.221781       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0627 05:42:14.224082       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0627 05:42:14.225286       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0627 05:42:14.231687       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0627 05:42:14.244058       1 shared_informer.go:357] "Caches are synced" controller="node"
I0627 05:42:14.244213       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0627 05:42:14.244334       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0627 05:42:14.244421       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0627 05:42:14.244444       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0627 05:42:14.253316       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0627 05:42:14.260990       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0627 05:42:14.261366       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0627 05:42:14.261470       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0627 05:42:14.261771       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0627 05:42:14.261371       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0627 05:42:14.261992       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0627 05:42:14.262112       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0627 05:42:14.262006       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0627 05:42:14.265283       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0627 05:42:14.265616       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0627 05:42:14.267204       1 shared_informer.go:357] "Caches are synced" controller="job"
I0627 05:42:14.269581       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0627 05:42:14.269628       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0627 05:42:14.312062       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0627 05:42:14.512340       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0627 05:42:14.514952       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0627 05:42:14.515163       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0627 05:42:14.520126       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0627 05:42:14.570090       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0627 05:42:14.570842       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0627 05:42:14.576757       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0627 05:42:14.581492       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0627 05:42:14.586060       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0627 05:42:14.597900       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0627 05:42:14.617990       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0627 05:42:14.978652       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0627 05:42:15.010524       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0627 05:42:15.010556       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0627 05:42:15.010564       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [53ddba9a2c19] <==
I0627 05:42:15.966675       1 server_linux.go:63] "Using iptables proxy"
I0627 05:42:16.117165       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0627 05:42:16.117218       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0627 05:42:16.131771       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0627 05:42:16.131804       1 server_linux.go:145] "Using iptables Proxier"
I0627 05:42:16.135208       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0627 05:42:16.139186       1 server.go:516] "Version info" version="v1.33.1"
I0627 05:42:16.139218       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0627 05:42:16.142689       1 config.go:199] "Starting service config controller"
I0627 05:42:16.142727       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0627 05:42:16.142959       1 config.go:105] "Starting endpoint slice config controller"
I0627 05:42:16.142992       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0627 05:42:16.143397       1 config.go:329] "Starting node config controller"
I0627 05:42:16.143418       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0627 05:42:16.143438       1 config.go:440] "Starting serviceCIDR config controller"
I0627 05:42:16.143441       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0627 05:42:16.243790       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0627 05:42:16.243830       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0627 05:42:16.243834       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0627 05:42:16.243809       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [a63c1ec9fabc] <==
I0627 05:42:05.342073       1 serving.go:386] Generated self-signed cert in-memory
W0627 05:42:07.621087       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0627 05:42:07.621239       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0627 05:42:07.621272       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0627 05:42:07.621338       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0627 05:42:07.723880       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0627 05:42:07.723968       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0627 05:42:07.726729       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0627 05:42:07.726784       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0627 05:42:07.726809       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0627 05:42:07.726812       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0627 05:42:07.728041       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0627 05:42:07.728249       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0627 05:42:07.728308       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0627 05:42:07.728335       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0627 05:42:07.728461       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0627 05:42:07.728601       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0627 05:42:07.728761       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0627 05:42:07.728804       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0627 05:42:07.728897       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0627 05:42:07.728966       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0627 05:42:07.728971       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0627 05:42:07.729010       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0627 05:42:07.729024       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0627 05:42:07.729028       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0627 05:42:07.729133       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0627 05:42:07.729161       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0627 05:42:08.552517       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0627 05:42:08.553111       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0627 05:42:08.577630       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0627 05:42:08.592128       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0627 05:42:08.612421       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0627 05:42:08.632176       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0627 05:42:08.708511       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0627 05:42:08.735462       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0627 05:42:08.824874       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0627 05:42:08.886138       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0627 05:42:11.325390       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123841    2531 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123846    2531 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123857    2531 policy_none.go:49] "None policy: Start"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123864    2531 memory_manager.go:186] "Starting memorymanager" policy="None"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123869    2531 state_mem.go:35] "Initializing new in-memory state store"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.123912    2531 state_mem.go:75] "Updated machine memory state"
Jun 27 05:42:10 minikube kubelet[2531]: E0627 05:42:10.124386    2531 manager.go:517] "Failed to read data from checkpoint" err="checkpoint is not found" checkpoint="kubelet_internal_checkpoint"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.124564    2531 eviction_manager.go:189] "Eviction manager: starting control loop"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.124576    2531 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.124759    2531 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jun 27 05:42:10 minikube kubelet[2531]: E0627 05:42:10.125338    2531 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.218749    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.218827    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.218859    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.218916    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.225754    2531 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.235281    2531 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.235361    2531 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279145    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279196    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279213    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279228    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279250    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/feee622ba49882ef945e2406d3ba86df-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"feee622ba49882ef945e2406d3ba86df\") " pod="kube-system/kube-scheduler-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279268    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279355    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279414    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279442    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279468    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279495    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279514    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279536    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279553    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-data\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jun 27 05:42:10 minikube kubelet[2531]: I0627 05:42:10.279575    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.067585    2531 apiserver.go:52] "Watching apiserver"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.076660    2531 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.144310    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.144570    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.144815    2531 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: E0627 05:42:11.204972    2531 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: E0627 05:42:11.205632    2531 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: E0627 05:42:11.206367    2531 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.221709    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.2216957019999999 podStartE2EDuration="1.221695702s" podCreationTimestamp="2025-06-27 05:42:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:11.221625392 +0000 UTC m=+1.205674505" watchObservedRunningTime="2025-06-27 05:42:11.221695702 +0000 UTC m=+1.205744797"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.229906    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.229891125 podStartE2EDuration="1.229891125s" podCreationTimestamp="2025-06-27 05:42:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:11.2298844 +0000 UTC m=+1.213933507" watchObservedRunningTime="2025-06-27 05:42:11.229891125 +0000 UTC m=+1.213940227"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.327461    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.327246788 podStartE2EDuration="1.327246788s" podCreationTimestamp="2025-06-27 05:42:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:11.323173411 +0000 UTC m=+1.307222639" watchObservedRunningTime="2025-06-27 05:42:11.327246788 +0000 UTC m=+1.311295960"
Jun 27 05:42:11 minikube kubelet[2531]: I0627 05:42:11.516823    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.5168037810000001 podStartE2EDuration="1.516803781s" podCreationTimestamp="2025-06-27 05:42:10 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:11.505127655 +0000 UTC m=+1.489176779" watchObservedRunningTime="2025-06-27 05:42:11.516803781 +0000 UTC m=+1.500852884"
Jun 27 05:42:14 minikube kubelet[2531]: I0627 05:42:14.325456    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sr42s\" (UniqueName: \"kubernetes.io/projected/6accb74d-1aeb-431d-9888-a890827b08d5-kube-api-access-sr42s\") pod \"storage-provisioner\" (UID: \"6accb74d-1aeb-431d-9888-a890827b08d5\") " pod="kube-system/storage-provisioner"
Jun 27 05:42:14 minikube kubelet[2531]: I0627 05:42:14.325533    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/6accb74d-1aeb-431d-9888-a890827b08d5-tmp\") pod \"storage-provisioner\" (UID: \"6accb74d-1aeb-431d-9888-a890827b08d5\") " pod="kube-system/storage-provisioner"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.278737    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=4.278721719 podStartE2EDuration="4.278721719s" podCreationTimestamp="2025-06-27 05:42:11 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:15.179630943 +0000 UTC m=+5.163680058" watchObservedRunningTime="2025-06-27 05:42:15.278721719 +0000 UTC m=+5.262770823"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.331808    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2120e59f-7d46-4d50-a0c4-acdfab4213fc-xtables-lock\") pod \"kube-proxy-jwxnm\" (UID: \"2120e59f-7d46-4d50-a0c4-acdfab4213fc\") " pod="kube-system/kube-proxy-jwxnm"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.331867    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2120e59f-7d46-4d50-a0c4-acdfab4213fc-lib-modules\") pod \"kube-proxy-jwxnm\" (UID: \"2120e59f-7d46-4d50-a0c4-acdfab4213fc\") " pod="kube-system/kube-proxy-jwxnm"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.331886    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r8pvp\" (UniqueName: \"kubernetes.io/projected/2120e59f-7d46-4d50-a0c4-acdfab4213fc-kube-api-access-r8pvp\") pod \"kube-proxy-jwxnm\" (UID: \"2120e59f-7d46-4d50-a0c4-acdfab4213fc\") " pod="kube-system/kube-proxy-jwxnm"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.331901    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/2120e59f-7d46-4d50-a0c4-acdfab4213fc-kube-proxy\") pod \"kube-proxy-jwxnm\" (UID: \"2120e59f-7d46-4d50-a0c4-acdfab4213fc\") " pod="kube-system/kube-proxy-jwxnm"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.533530    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/89a253e5-6296-4480-bed8-25ee6a9de709-config-volume\") pod \"coredns-674b8bbfcf-scg9h\" (UID: \"89a253e5-6296-4480-bed8-25ee6a9de709\") " pod="kube-system/coredns-674b8bbfcf-scg9h"
Jun 27 05:42:15 minikube kubelet[2531]: I0627 05:42:15.533643    2531 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q2zv8\" (UniqueName: \"kubernetes.io/projected/89a253e5-6296-4480-bed8-25ee6a9de709-kube-api-access-q2zv8\") pod \"coredns-674b8bbfcf-scg9h\" (UID: \"89a253e5-6296-4480-bed8-25ee6a9de709\") " pod="kube-system/coredns-674b8bbfcf-scg9h"
Jun 27 05:42:16 minikube kubelet[2531]: I0627 05:42:16.192187    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-scg9h" podStartSLOduration=1.192175055 podStartE2EDuration="1.192175055s" podCreationTimestamp="2025-06-27 05:42:15 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:16.191749987 +0000 UTC m=+6.175799106" watchObservedRunningTime="2025-06-27 05:42:16.192175055 +0000 UTC m=+6.176224152"
Jun 27 05:42:16 minikube kubelet[2531]: I0627 05:42:16.209712    2531 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-jwxnm" podStartSLOduration=1.209697404 podStartE2EDuration="1.209697404s" podCreationTimestamp="2025-06-27 05:42:15 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-27 05:42:16.209659804 +0000 UTC m=+6.193708906" watchObservedRunningTime="2025-06-27 05:42:16.209697404 +0000 UTC m=+6.193746509"
Jun 27 05:42:20 minikube kubelet[2531]: I0627 05:42:20.282556    2531 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 27 05:42:20 minikube kubelet[2531]: I0627 05:42:20.284652    2531 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 27 05:42:23 minikube kubelet[2531]: I0627 05:42:23.042586    2531 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jun 27 05:42:36 minikube kubelet[2531]: I0627 05:42:36.353971    2531 scope.go:117] "RemoveContainer" containerID="aa3c63239adfd3644aa070b3e1ec090ea61a555c615038a3be6b4960e9751ade"


==> storage-provisioner [8b5621dd3089] <==
W0627 05:44:47.102054       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:47.105142       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:49.112359       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:49.121378       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:51.125355       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:51.130280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:53.136875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:53.144661       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:55.150613       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:55.156323       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:57.164032       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:57.170188       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:59.175000       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:44:59.180322       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:01.186584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:01.192333       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:03.197498       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:03.203251       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:05.206159       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:05.210857       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:07.213899       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:07.217082       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:09.219161       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:09.222530       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:11.225148       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:11.228587       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:13.233864       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:13.239362       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:15.243874       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:15.248797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:17.256364       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:17.262152       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:19.266658       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:19.271011       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:21.277483       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:21.285688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:23.291535       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:23.297370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:25.304375       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:25.310470       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:27.314871       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:27.319204       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:29.325470       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:29.333571       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:31.339726       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:31.349564       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:33.355309       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:33.361551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:35.367936       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:35.375490       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:37.382740       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:37.389282       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:39.391770       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:39.396952       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:41.402094       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:41.408432       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:43.413052       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:43.418975       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:45.421978       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0627 05:45:45.426029       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [aa3c63239adf] <==
I0627 05:42:14.847236       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0627 05:42:35.892038       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

